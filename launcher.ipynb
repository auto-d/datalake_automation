{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e00f2c-5970-43b8-af4f-159f57ffa655",
   "metadata": {},
   "source": [
    "# Datalake Setup Automation \n",
    "## Overview\n",
    "\n",
    "**Requirements**\n",
    "- Download a large dataset using a virtual machine\n",
    "- Create a data lake to upload the dataset to in a cloud platform of your choice\n",
    "- No local downloads, must use a cloud VM and document the process via screenshare\n",
    "- Walkthrough of the process via <5 minute video and upload\n",
    "\n",
    "**Rubric**\n",
    "- Video is <5 minutes\n",
    "- Video walks through process clearly \n",
    "- Team uses a virtual machine\n",
    "- Team does not download any data locally\n",
    "- Team creates a data lake in a cloud platform\n",
    "- Team downloads/uploads dataset to the data lake using the VM\n",
    "\n",
    "## Suggested Approach \n",
    "\n",
    "Use an AWS Sagemaker Studio Jupyter interface to host this Jupyter notebook, leveraging the boto3 library to interact with AWS EC2, S3 to satisfy the assignment requirements. This has the benefit of: \n",
    "- minimizing the clicking around in the Web UI\n",
    "- furnishing code that can be used later in the class with other datasets, where running this repeatedly will be annoying in the GUI\n",
    "\n",
    "**Steps**\n",
    "1. Load this Jupyter notebook into an AWS SageMaker lab instance\n",
    "2. Ensure [`boto3`](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html) is installed \n",
    "3. Configure boto with necessary credentials for S3 access\n",
    "these steps over and over again may become tedious\n",
    "5. Create the S3 bucket that will serve as a data lake for our unstructured data\n",
    "6. Use Requests or similar to grab a pile of data, write it to disk if we need an intermediate stage (could be essential if downloads are interrupted or lengthy) \n",
    "7. Upload the data to S3 and validate it's presence\n",
    "8. Destroy local data and release Sagemaker resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62da6890-6e22-414a-ae58-e51dc88ed4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be preinstalled in the env if we run in Sagemaker studio\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa39efb5-b03b-4d52-8704-a887b9d10aac",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0dbd5-98fd-4e13-93de-97fff2fc7be1",
   "metadata": {},
   "source": [
    "Check to ensure AWS configuration exists, write it to disk if it doesn't... this might not be necessary from a Sagemaker instance context... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7d8a1-ccb2-45c6-bee7-286abe4fc5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = \"\"\"\\\n",
    "[default]\\\n",
    "aws_access_key_id = YOUR_ACCESS_KEY\\\n",
    "aws_secret_access_key = YOUR_SECRET_KEY\\\n",
    "\"\"\"\n",
    "\n",
    "aws_creds_file = \"~/.aws/credentials\"\n",
    "\n",
    "if not ~/.aws/credentials: \n",
    "    write default_config to aws_credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48efb0-d65c-4b4c-b54d-0706fb5baeed",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "Go fetch the dataset(s) that will be uploaded to S3 to create our data lake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f6c34-4145-4232-952d-bee30b79fdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will be preinstalled in the env if we run in Sagemaker studio\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f079b-10b8-4964-bc20-501c9bab4623",
   "metadata": {},
   "source": [
    "## Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825bfd0-ad8d-4693-a3e6-4760f2707aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Team Assignments",
   "language": "python",
   "name": "team-assignments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
